{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4 Framework.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOUbyu1sJX13vjSmnQsiPK5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villafue/Capstone_2_MovieLens/blob/main/Notebook/4_Framework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1IxRBsREbjO"
      },
      "source": [
        "# 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m783EMJiEjhG"
      },
      "source": [
        "<a name=\"top\"></a>\n",
        "## Framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WasouTHJILH"
      },
      "source": [
        "### Table of Contents\n",
        "\n",
        "Note: The internal links work when the notebook is run via Google Colab.\n",
        "\n",
        "1. **[Preface](#preface)**\n",
        "2. **[Introduction](#introduction)**\n",
        "3. **[Exploratory Data Analysis](#eda)**\n",
        "    - 3.1 - [Import Packages](https://colab.research.google.com/github/villafue/Capstone_2_MovieLens/blob/main/Notebook/3_Exploratory_Data_Analysis.ipynb#import_packages)\n",
        "    - 3.2 - [Movie Lens Data](https://colab.research.google.com/github/villafue/Capstone_2_MovieLens/blob/main/Notebook/3_Exploratory_Data_Analysis.ipynb#data)\n",
        "        - 3.2.1 - [Links.csv](https://colab.research.google.com/github/villafue/Capstone_2_MovieLens/blob/main/Notebook/3_Exploratory_Data_Analysis.ipynb#links)\n",
        "        - 3.2.2 - [Tags.csv](https://colab.research.google.com/github/villafue/Capstone_2_MovieLens/blob/main/Notebook/3_Exploratory_Data_Analysis.ipynb#tags)\n",
        "        - 3.2.3 - [Movies.csv](https://colab.research.google.com/github/villafue/Capstone_2_MovieLens/blob/main/Notebook/3_Exploratory_Data_Analysis.ipynb#movies)\n",
        "        - 3.2.4 - [Ratings.csv](https://colab.research.google.com/github/villafue/Capstone_2_MovieLens/blob/main/Notebook/3_Exploratory_Data_Analysis.ipynb#ratings)\n",
        "    - 3.3 - [Movies and Ratings Analysis](https://colab.research.google.com/github/villafue/Capstone_2_MovieLens/blob/main/Notebook/3_Exploratory_Data_Analysis.ipynb#movies_ratings)\n",
        "        - 3.3.1 - [Merge Movies and Ratings](https://colab.research.google.com/github/villafue/Capstone_2_MovieLens/blob/main/Notebook/3_Exploratory_Data_Analysis.ipynb#merge)\n",
        "        - 3.3.2 - [Investigating Rating Count](https://colab.research.google.com/github/villafue/Capstone_2_MovieLens/blob/main/Notebook/3_Exploratory_Data_Analysis.ipynb#rating_count)\n",
        "        - 3.3.3 - [Highest Rated Movies](https://colab.research.google.com/github/villafue/Capstone_2_MovieLens/blob/main/Notebook/3_Exploratory_Data_Analysis.ipynb#highest_rated)\n",
        "    - 3.4 - [Genre](https://colab.research.google.com/github/villafue/Capstone_2_MovieLens/blob/main/Notebook/3_Exploratory_Data_Analysis.ipynb#genre)\n",
        "    - 3.5 - [User Rating Count](https://colab.research.google.com/github/villafue/Capstone_2_MovieLens/blob/main/Notebook/3_Exploratory_Data_Analysis.ipynb#user_rating_count)\n",
        "    - 3.6 - [User 25](https://colab.research.google.com/github/villafue/Capstone_2_MovieLens/blob/main/Notebook/3_Exploratory_Data_Analysis.ipynb#user_25)\n",
        "4. **[Framework](#framework)**\n",
        "    - 4.1 - [Introduction](#introduction)\n",
        "    - 4.2 - [RecommenderMetrics.py](#recommendermetrics)\n",
        "    - 4.3 - [MovieLens.py](#movielens)\n",
        "    - 4.4 - [EvaluatedAlgorithm.py](#evaluatedalgorithm)\n",
        "    - 4.5 - [EvaluationData.py](#evaluationdata)\n",
        "    - 4.6 - [Evaluator.py](#evaluator)\n",
        "    - 4.7 - [Conclusion](#conclusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQY-F0vOcQb9"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th0Sq1sPeaIO"
      },
      "source": [
        "<a name=\"introduction\"></a>\n",
        "### 4.1 - Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVytJyi1FPeD"
      },
      "source": [
        "This Notebook's goal is to give a brief introduction to Frank's core framework. He wrote several custom python scripts to make it easier to grasp the different concepts for recommender systems, and to automate the execution. It includes five interpendent python files that serve as the foundation for the recommendation systems.\n",
        "\n",
        "Each of the subsesquent Notebooks include these 5 scripts, as well as others specific to the type of recommendation system. Furthermore, I had to adapt his some of his code to make it work for my needs. Links to the exact scripts will be provided in the subsequent modeling notebooks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9wkWFNcjaI1"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR-icWtwghny"
      },
      "source": [
        "**[Back to Top](#top)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYvHDd1XgiiZ"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PjROgz0Yg-U"
      },
      "source": [
        "<a name=\"recommendermetrics\"></a>\n",
        "### 4.2 - RecommenderMetrics.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQWdctKBcSFT"
      },
      "source": [
        "RecommenderMetrics.py is the script that quickly calculates the different metrics we use to assess our models. \n",
        "\n",
        "It uses both Surpriselib's [Accuracy module](https://surprise.readthedocs.io/en/stable/accuracy.html#) as well as other custom metrics written for this course. Here is a quick legend that prints out when the models finish training, and also explains what each of the metrics measure:\n",
        "\n",
        "```\n",
        "Legend:\n",
        "\n",
        "RMSE:      Root Mean Squared Error. Lower values mean better accuracy.\n",
        "MAE:       Mean Absolute Error. Lower values mean better accuracy.\n",
        "HR:        Hit Rate; how often we are able to recommend a left-out rating. Higher is better.\n",
        "cHR:       Cumulative Hit Rate; hit rate, confined to ratings above a certain threshold. Higher is better.\n",
        "ARHR:      Average Reciprocal Hit Rank - Hit rate that takes the ranking into account. Higher is better.\n",
        "Coverage:  Ratio of users for whom recommendations above a certain threshold exist. Higher is better.\n",
        "Diversity: 1-S, where S is the average similarity score between every possible pair of recommendations\n",
        "           for a given user. Higher means more diverse.\n",
        "Novelty:   Average popularity rank of recommended items. Higher means more novel.\n",
        "```\n",
        "\n",
        "All of these metrics are coded into this script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o79VuTZsbnKj"
      },
      "source": [
        "import itertools\n",
        "\n",
        "from surprise import accuracy\n",
        "from collections import defaultdict\n",
        "\n",
        "class RecommenderMetrics:\n",
        "\n",
        "    def MAE(predictions):\n",
        "        return accuracy.mae(predictions, verbose=False)\n",
        "\n",
        "    def RMSE(predictions):\n",
        "        return accuracy.rmse(predictions, verbose=False)\n",
        "\n",
        "    def GetTopN(predictions, n=10, minimumRating=4.0):\n",
        "        topN = defaultdict(list)\n",
        "\n",
        "\n",
        "        for userID, movieID, actualRating, estimatedRating, _ in predictions:\n",
        "            if (estimatedRating >= minimumRating):\n",
        "                topN[int(userID)].append((int(movieID), estimatedRating))\n",
        "\n",
        "        for userID, ratings in topN.items():\n",
        "            ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "            topN[int(userID)] = ratings[:n]\n",
        "\n",
        "        return topN\n",
        "\n",
        "    def HitRate(topNPredicted, leftOutPredictions):\n",
        "        hits = 0\n",
        "        total = 0\n",
        "\n",
        "        # For each left-out rating\n",
        "        for leftOut in leftOutPredictions:\n",
        "            userID = leftOut[0]\n",
        "            leftOutMovieID = leftOut[1]\n",
        "            # Is it in the predicted top 10 for this user?\n",
        "            hit = False\n",
        "            for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "                if (int(leftOutMovieID) == int(movieID)):\n",
        "                    hit = True\n",
        "                    break\n",
        "            if (hit) :\n",
        "                hits += 1\n",
        "\n",
        "            total += 1\n",
        "\n",
        "        # Compute overall precision\n",
        "        return hits/total\n",
        "\n",
        "    def CumulativeHitRate(topNPredicted, leftOutPredictions, ratingCutoff=0):\n",
        "        hits = 0\n",
        "        total = 0\n",
        "\n",
        "        # For each left-out rating\n",
        "        for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:\n",
        "            # Only look at ability to recommend things the users actually liked...\n",
        "            if (actualRating >= ratingCutoff):\n",
        "                # Is it in the predicted top 10 for this user?\n",
        "                hit = False\n",
        "                for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "                    if (int(leftOutMovieID) == movieID):\n",
        "                        hit = True\n",
        "                        break\n",
        "                if (hit) :\n",
        "                    hits += 1\n",
        "\n",
        "                total += 1\n",
        "\n",
        "        # Compute overall precision\n",
        "        return hits/total\n",
        "\n",
        "    def RatingHitRate(topNPredicted, leftOutPredictions):\n",
        "        hits = defaultdict(float)\n",
        "        total = defaultdict(float)\n",
        "\n",
        "        # For each left-out rating\n",
        "        for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:\n",
        "            # Is it in the predicted top N for this user?\n",
        "            hit = False\n",
        "            for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "                if (int(leftOutMovieID) == movieID):\n",
        "                    hit = True\n",
        "                    break\n",
        "            if (hit) :\n",
        "                hits[actualRating] += 1\n",
        "\n",
        "            total[actualRating] += 1\n",
        "\n",
        "        # Compute overall precision\n",
        "        for rating in sorted(hits.keys()):\n",
        "            print (rating, hits[rating] / total[rating])\n",
        "\n",
        "    def AverageReciprocalHitRank(topNPredicted, leftOutPredictions):\n",
        "        summation = 0\n",
        "        total = 0\n",
        "        # For each left-out rating\n",
        "        for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:\n",
        "            # Is it in the predicted top N for this user?\n",
        "            hitRank = 0\n",
        "            rank = 0\n",
        "            for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "                rank = rank + 1\n",
        "                if (int(leftOutMovieID) == movieID):\n",
        "                    hitRank = rank\n",
        "                    break\n",
        "            if (hitRank > 0) :\n",
        "                summation += 1.0 / hitRank\n",
        "\n",
        "            total += 1\n",
        "\n",
        "        return summation / total\n",
        "\n",
        "    # What percentage of users have at least one \"good\" recommendation\n",
        "    def UserCoverage(topNPredicted, numUsers, ratingThreshold=0):\n",
        "        hits = 0\n",
        "        for userID in topNPredicted.keys():\n",
        "            hit = False\n",
        "            for movieID, predictedRating in topNPredicted[userID]:\n",
        "                if (predictedRating >= ratingThreshold):\n",
        "                    hit = True\n",
        "                    break\n",
        "            if (hit):\n",
        "                hits += 1\n",
        "\n",
        "        return hits / numUsers\n",
        "\n",
        "    def Diversity(topNPredicted, simsAlgo):\n",
        "        n = 0\n",
        "        total = 0\n",
        "        simsMatrix = simsAlgo.compute_similarities()\n",
        "        for userID in topNPredicted.keys():\n",
        "            pairs = itertools.combinations(topNPredicted[userID], 2)\n",
        "            for pair in pairs:\n",
        "                movie1 = pair[0][0]\n",
        "                movie2 = pair[1][0]\n",
        "                innerID1 = simsAlgo.trainset.to_inner_iid(str(movie1))\n",
        "                innerID2 = simsAlgo.trainset.to_inner_iid(str(movie2))\n",
        "                similarity = simsMatrix[innerID1][innerID2]\n",
        "                total += similarity\n",
        "                n += 1\n",
        "\n",
        "        S = total / n\n",
        "        return (1-S)\n",
        "\n",
        "    def Novelty(topNPredicted, rankings):\n",
        "        n = 0\n",
        "        total = 0\n",
        "        for userID in topNPredicted.keys():\n",
        "            for rating in topNPredicted[userID]:\n",
        "                movieID = rating[0]\n",
        "                rank = rankings[movieID]\n",
        "                total += rank\n",
        "                n += 1\n",
        "        return total / n\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnPTwWTmccxV"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vuoqt75rccxV"
      },
      "source": [
        "**[Back to Top](#top)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8gvTIHAccxX"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdRrWE6_hzO8"
      },
      "source": [
        "<a name=\"movielens\"></a>\n",
        "### 4.3 - MovieLens.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qucCdMKzTD6"
      },
      "source": [
        "The MovieLens.py script has several functions for loading the data from both ratings.csv and movies.csv. These functions are used in other modules and automates the cleaning and organizing of the information.\n",
        "\n",
        "Note that on Line 22, I commented out the code when I adapted it for use on Google Colab. Furthermore, on lines 28 and 30 use special functions from surpriselib's [Dataset module](https://surprise.readthedocs.io/en/stable/dataset.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgZm9dGhy_5Y"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import sys\n",
        "import re\n",
        "\n",
        "from surprise import Dataset\n",
        "from surprise import Reader\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "class MovieLens:\n",
        "\n",
        "    movieID_to_name = {}\n",
        "    name_to_movieID = {}\n",
        "    ratingsPath = 'ratings.csv'\n",
        "    moviesPath = 'movies.csv'\n",
        "    \n",
        "    def loadMovieLensLatestSmall(self):\n",
        "\n",
        "        # Look for files relative to the directory we are running from\n",
        "        #os.chdir(os.path.dirname(sys.argv[0]))\n",
        "\n",
        "        ratingsDataset = 0\n",
        "        self.movieID_to_name = {}\n",
        "        self.name_to_movieID = {}\n",
        "\n",
        "        reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)\n",
        "\n",
        "        ratingsDataset = Dataset.load_from_file(self.ratingsPath, reader=reader)\n",
        "\n",
        "        with open(self.moviesPath, newline='', encoding='ISO-8859-1') as csvfile:\n",
        "                movieReader = csv.reader(csvfile)\n",
        "                next(movieReader)  #Skip header line\n",
        "                for row in movieReader:\n",
        "                    movieID = int(row[0])\n",
        "                    movieName = row[1]\n",
        "                    self.movieID_to_name[movieID] = movieName\n",
        "                    self.name_to_movieID[movieName] = movieID\n",
        "\n",
        "        return ratingsDataset\n",
        "\n",
        "    def getUserRatings(self, user):\n",
        "        userRatings = []\n",
        "        hitUser = False\n",
        "        with open(self.ratingsPath, newline='') as csvfile:\n",
        "            ratingReader = csv.reader(csvfile)\n",
        "            next(ratingReader)\n",
        "            for row in ratingReader:\n",
        "                userID = int(row[0])\n",
        "                if (user == userID):\n",
        "                    movieID = int(row[1])\n",
        "                    rating = float(row[2])\n",
        "                    userRatings.append((movieID, rating))\n",
        "                    hitUser = True\n",
        "                if (hitUser and (user != userID)):\n",
        "                    break\n",
        "\n",
        "        return userRatings\n",
        "\n",
        "    def getPopularityRanks(self):\n",
        "        ratings = defaultdict(int)\n",
        "        rankings = defaultdict(int)\n",
        "        with open(self.ratingsPath, newline='') as csvfile:\n",
        "            ratingReader = csv.reader(csvfile)\n",
        "            next(ratingReader)\n",
        "            for row in ratingReader:\n",
        "                movieID = int(row[1])\n",
        "                ratings[movieID] += 1\n",
        "        rank = 1\n",
        "        for movieID, ratingCount in sorted(ratings.items(), key=lambda x: x[1], reverse=True):\n",
        "            rankings[movieID] = rank\n",
        "            rank += 1\n",
        "        return rankings\n",
        "    \n",
        "    def getGenres(self):\n",
        "        genres = defaultdict(list)\n",
        "        genreIDs = {}\n",
        "        maxGenreID = 0\n",
        "        with open(self.moviesPath, newline='', encoding='ISO-8859-1') as csvfile:\n",
        "            movieReader = csv.reader(csvfile)\n",
        "            next(movieReader)  #Skip header line\n",
        "            for row in movieReader:\n",
        "                movieID = int(row[0])\n",
        "                genreList = row[2].split('|')\n",
        "                genreIDList = []\n",
        "                for genre in genreList:\n",
        "                    if genre in genreIDs:\n",
        "                        genreID = genreIDs[genre]\n",
        "                    else:\n",
        "                        genreID = maxGenreID\n",
        "                        genreIDs[genre] = genreID\n",
        "                        maxGenreID += 1\n",
        "                    genreIDList.append(genreID)\n",
        "                genres[movieID] = genreIDList\n",
        "        # Convert integer-encoded genre lists to bitfields that we can treat as vectors\n",
        "        for (movieID, genreIDList) in genres.items():\n",
        "            bitfield = [0] * maxGenreID\n",
        "            for genreID in genreIDList:\n",
        "                bitfield[genreID] = 1\n",
        "            genres[movieID] = bitfield            \n",
        "        \n",
        "        return genres\n",
        "    \n",
        "    def getYears(self):\n",
        "        p = re.compile(r\"(?:\\((\\d{4})\\))?\\s*$\")\n",
        "        years = defaultdict(int)\n",
        "        with open(self.moviesPath, newline='', encoding='ISO-8859-1') as csvfile:\n",
        "            movieReader = csv.reader(csvfile)\n",
        "            next(movieReader)\n",
        "            for row in movieReader:\n",
        "                movieID = int(row[0])\n",
        "                title = row[1]\n",
        "                m = p.search(title)\n",
        "                year = m.group(1)\n",
        "                if year:\n",
        "                    years[movieID] = int(year)\n",
        "        return years\n",
        "    \n",
        "    def getMiseEnScene(self):\n",
        "        mes = defaultdict(list)\n",
        "        with open(\"LLVisualFeatures13K_Log.csv\", newline='') as csvfile:\n",
        "            mesReader = csv.reader(csvfile)\n",
        "            next(mesReader)\n",
        "            for row in mesReader:\n",
        "                movieID = int(row[0])\n",
        "                avgShotLength = float(row[1])\n",
        "                meanColorVariance = float(row[2])\n",
        "                stddevColorVariance = float(row[3])\n",
        "                meanMotion = float(row[4])\n",
        "                stddevMotion = float(row[5])\n",
        "                meanLightingKey = float(row[6])\n",
        "                numShots = float(row[7])\n",
        "                mes[movieID] = [avgShotLength, meanColorVariance, stddevColorVariance,\n",
        "                   meanMotion, stddevMotion, meanLightingKey, numShots]\n",
        "        return mes\n",
        "    \n",
        "    def getMovieName(self, movieID):\n",
        "        if movieID in self.movieID_to_name:\n",
        "            return self.movieID_to_name[movieID]\n",
        "        else:\n",
        "            return \"\"\n",
        "        \n",
        "    def getMovieID(self, movieName):\n",
        "        if movieName in self.name_to_movieID:\n",
        "            return self.name_to_movieID[movieName]\n",
        "        else:\n",
        "            return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vlq-wJLZh8zG"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssbndqhoh8zH"
      },
      "source": [
        "**[Back to Top](#top)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7OmpRbVh8zH"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdPNe30qh8zI"
      },
      "source": [
        "<a name=\"evaluatedalgorithm\"></a>\n",
        "### 4.4 - EvaluatedAlgorithm.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxL9KJd73DPi"
      },
      "source": [
        "The EvaluatedAlgorithm Class is what is called to use all the metrics defined in the RecommenderMetrics Class. It's this script that prints out the \"Legend\" shown back in [section 4.2](#recommendermetrics). Furthermore, the recommender models will always print out both the MAE and RMSE (lines 23 and 24), while the other metrics are an optional. This is to save time as these models easily can take at least 2 hours to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT2R2jgg3KSl"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu May  3 10:45:33 2018\n",
        "\n",
        "@author: Frank\n",
        "\"\"\"\n",
        "from RecommenderMetrics import RecommenderMetrics\n",
        "from EvaluationData import EvaluationData\n",
        "\n",
        "class EvaluatedAlgorithm:\n",
        "    \n",
        "    def __init__(self, algorithm, name):\n",
        "        self.algorithm = algorithm\n",
        "        self.name = name\n",
        "        \n",
        "    def Evaluate(self, evaluationData, doTopN, n=10, verbose=True):\n",
        "        metrics = {}\n",
        "        # Compute accuracy\n",
        "        if (verbose):\n",
        "            print(\"Evaluating accuracy...\")\n",
        "        self.algorithm.fit(evaluationData.GetTrainSet())\n",
        "        predictions = self.algorithm.test(evaluationData.GetTestSet())\n",
        "        metrics[\"RMSE\"] = RecommenderMetrics.RMSE(predictions)\n",
        "        metrics[\"MAE\"] = RecommenderMetrics.MAE(predictions)\n",
        "        \n",
        "        if (doTopN):\n",
        "            # Evaluate top-10 with Leave One Out testing\n",
        "            if (verbose):\n",
        "                print(\"Evaluating top-N with leave-one-out...\")\n",
        "            self.algorithm.fit(evaluationData.GetLOOCVTrainSet())\n",
        "            leftOutPredictions = self.algorithm.test(evaluationData.GetLOOCVTestSet())        \n",
        "            # Build predictions for all ratings not in the training set\n",
        "            allPredictions = self.algorithm.test(evaluationData.GetLOOCVAntiTestSet())\n",
        "            # Compute top 10 recs for each user\n",
        "            topNPredicted = RecommenderMetrics.GetTopN(allPredictions, n)\n",
        "            if (verbose):\n",
        "                print(\"Computing hit-rate and rank metrics...\")\n",
        "            # See how often we recommended a movie the user actually rated\n",
        "            metrics[\"HR\"] = RecommenderMetrics.HitRate(topNPredicted, leftOutPredictions)   \n",
        "            # See how often we recommended a movie the user actually liked\n",
        "            metrics[\"cHR\"] = RecommenderMetrics.CumulativeHitRate(topNPredicted, leftOutPredictions)\n",
        "            # Compute ARHR\n",
        "            metrics[\"ARHR\"] = RecommenderMetrics.AverageReciprocalHitRank(topNPredicted, leftOutPredictions)\n",
        "        \n",
        "            #Evaluate properties of recommendations on full training set\n",
        "            if (verbose):\n",
        "                print(\"Computing recommendations with full data set...\")\n",
        "            self.algorithm.fit(evaluationData.GetFullTrainSet())\n",
        "            allPredictions = self.algorithm.test(evaluationData.GetFullAntiTestSet())\n",
        "            topNPredicted = RecommenderMetrics.GetTopN(allPredictions, n)\n",
        "            if (verbose):\n",
        "                print(\"Analyzing coverage, diversity, and novelty...\")\n",
        "            # Print user coverage with a minimum predicted rating of 4.0:\n",
        "            metrics[\"Coverage\"] = RecommenderMetrics.UserCoverage(  topNPredicted, \n",
        "                                                                   evaluationData.GetFullTrainSet().n_users, \n",
        "                                                                   ratingThreshold=4.0)\n",
        "            # Measure diversity of recommendations:\n",
        "            metrics[\"Diversity\"] = RecommenderMetrics.Diversity(topNPredicted, evaluationData.GetSimilarities())\n",
        "            \n",
        "            # Measure novelty (average popularity rank of recommendations):\n",
        "            metrics[\"Novelty\"] = RecommenderMetrics.Novelty(topNPredicted, \n",
        "                                                            evaluationData.GetPopularityRankings())\n",
        "        \n",
        "        if (verbose):\n",
        "            print(\"Analysis complete.\")\n",
        "    \n",
        "        return metrics\n",
        "    \n",
        "    def GetName(self):\n",
        "        return self.name\n",
        "    \n",
        "    def GetAlgorithm(self):\n",
        "        return self.algorithm\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnq_xER03Deo"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQhTLdmO3Deo"
      },
      "source": [
        "**[Back to Top](#top)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84HVoA8l3Dep"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR_sBi9N3Dep"
      },
      "source": [
        "<a name=\"evaluationdata\"></a>\n",
        "### 4.5 - EvaluationData.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKohD1UD-I82"
      },
      "source": [
        "The EvaluationData class is what pre-processes the data for modeling. It includes SurpriseLib's SciKit-Learn inspired [train_test_split and LeaveOneOut](https://surprise.readthedocs.io/en/stable/model_selection.html) cross validation strategies. It also uses Surpriselib functions such as `build_anti_testset()` and `build_full_trainset()` in the [Trainset class](https://surprise.readthedocs.io/en/stable/trainset.html). On lines 34-36, this script computes a similarity matrix with Surpriselib's [KNNBaseline](https://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNBaseline) algorithm. The similarity matrix is used for many measures to include diversity. With this, all the data is automated and ready to train our recommendation systems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09CYMIFq-LRW"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu May  3 10:48:02 2018\n",
        "\n",
        "@author: Frank\n",
        "\"\"\"\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise.model_selection import LeaveOneOut\n",
        "from surprise import KNNBaseline\n",
        "\n",
        "class EvaluationData:\n",
        "    \n",
        "    def __init__(self, data, popularityRankings):\n",
        "        \n",
        "        self.rankings = popularityRankings\n",
        "        \n",
        "        #Build a full training set for evaluating overall properties\n",
        "        self.fullTrainSet = data.build_full_trainset()\n",
        "        self.fullAntiTestSet = self.fullTrainSet.build_anti_testset()\n",
        "        \n",
        "        #Build a 75/25 train/test split for measuring accuracy\n",
        "        self.trainSet, self.testSet = train_test_split(data, test_size=.25, random_state=1)\n",
        "        \n",
        "        #Build a \"leave one out\" train/test split for evaluating top-N recommenders\n",
        "        #And build an anti-test-set for building predictions\n",
        "        LOOCV = LeaveOneOut(n_splits=1, random_state=1)\n",
        "        for train, test in LOOCV.split(data):\n",
        "            self.LOOCVTrain = train\n",
        "            self.LOOCVTest = test\n",
        "            \n",
        "        self.LOOCVAntiTestSet = self.LOOCVTrain.build_anti_testset()\n",
        "        \n",
        "        #Compute similarty matrix between items so we can measure diversity\n",
        "        sim_options = {'name': 'cosine', 'user_based': False}\n",
        "        self.simsAlgo = KNNBaseline(sim_options=sim_options)\n",
        "        self.simsAlgo.fit(self.fullTrainSet)\n",
        "            \n",
        "    def GetFullTrainSet(self):\n",
        "        return self.fullTrainSet\n",
        "    \n",
        "    def GetFullAntiTestSet(self):\n",
        "        return self.fullAntiTestSet\n",
        "    \n",
        "    def GetAntiTestSetForUser(self, testSubject):\n",
        "        trainset = self.fullTrainSet\n",
        "        fill = trainset.global_mean\n",
        "        anti_testset = []\n",
        "        u = trainset.to_inner_uid(str(testSubject))\n",
        "        user_items = set([j for (j, _) in trainset.ur[u]])\n",
        "        anti_testset += [(trainset.to_raw_uid(u), trainset.to_raw_iid(i), fill) for\n",
        "                                 i in trainset.all_items() if\n",
        "                                 i not in user_items]\n",
        "        return anti_testset\n",
        "\n",
        "    def GetTrainSet(self):\n",
        "        return self.trainSet\n",
        "    \n",
        "    def GetTestSet(self):\n",
        "        return self.testSet\n",
        "    \n",
        "    def GetLOOCVTrainSet(self):\n",
        "        return self.LOOCVTrain\n",
        "    \n",
        "    def GetLOOCVTestSet(self):\n",
        "        return self.LOOCVTest\n",
        "    \n",
        "    def GetLOOCVAntiTestSet(self):\n",
        "        return self.LOOCVAntiTestSet\n",
        "    \n",
        "    def GetSimilarities(self):\n",
        "        return self.simsAlgo\n",
        "    \n",
        "    def GetPopularityRankings(self):\n",
        "        return self.rankings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5MNfneU-JOo"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9vHvkXy-JOo"
      },
      "source": [
        "**[Back to Top](#top)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTXrc94L-JOo"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDBig3lE-JOp"
      },
      "source": [
        "<a name=\"evaluator\"></a>\n",
        "### 4.6 - Evaluator.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kcoVdGmLYUI"
      },
      "source": [
        "The Evaluator Class makes it easy to quickly add the different algorithms to train and test at the same time. It's the highest level interface that automatically use and run all the other scripts. Line 18 `AddAlgorithm` is the simple method that adds any model we want to train. The `Evaluate` method on line 22 gives us some flexibility on what metrics we want to use to evaluate the models. It always shows the RMSE and MAE, but when the argument is set to `True`, it calculates all the metrics and prints them out along with the legend. Here is an [example](#recommendermetrics).\n",
        "\n",
        "The method on line 55 `SampleTopNRecs` is how we retreive our top 10 recommendations for a specific user. `testSubject=25` specifies that I want to print out the top ten recommendations for User 25. I can change the user to anyone I desire. This is an example of the recommendations the model outputs:\n",
        "\n",
        "```\n",
        "We recommend:\n",
        "Usual Suspects, The (1995) 5\n",
        "Forrest Gump (1994) 5\n",
        "Silence of the Lambs, The (1991) 5\n",
        "Star Wars: Episode V - The Empire Strikes Back (1980) 5\n",
        "Princess Bride, The (1987) 5\n",
        "Goodfellas (1990) 5\n",
        "Shining, The (1980) 5\n",
        "American History X (1998) 5\n",
        "Fight Club (1999) 5\n",
        "Shawshank Redemption, The (1994) 5\n",
        "```\n",
        "The Evaluator is arguably the interface for framework.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1Uger1-LZZg"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu May  3 10:22:34 2018\n",
        "\n",
        "@author: Frank\n",
        "\"\"\"\n",
        "from EvaluationData import EvaluationData\n",
        "from EvaluatedAlgorithm import EvaluatedAlgorithm\n",
        "\n",
        "class Evaluator:\n",
        "    \n",
        "    algorithms = []\n",
        "    \n",
        "    def __init__(self, dataset, rankings):\n",
        "        ed = EvaluationData(dataset, rankings)\n",
        "        self.dataset = ed\n",
        "        \n",
        "    def AddAlgorithm(self, algorithm, name):\n",
        "        alg = EvaluatedAlgorithm(algorithm, name)\n",
        "        self.algorithms.append(alg)\n",
        "        \n",
        "    def Evaluate(self, doTopN):\n",
        "        results = {}\n",
        "        for algorithm in self.algorithms:\n",
        "            print(\"Evaluating \", algorithm.GetName(), \"...\")\n",
        "            results[algorithm.GetName()] = algorithm.Evaluate(self.dataset, doTopN)\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\n\")\n",
        "        \n",
        "        if (doTopN):\n",
        "            print(\"{:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
        "                    \"Algorithm\", \"RMSE\", \"MAE\", \"HR\", \"cHR\", \"ARHR\", \"Coverage\", \"Diversity\", \"Novelty\"))\n",
        "            for (name, metrics) in results.items():\n",
        "                print(\"{:<10} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(\n",
        "                        name, metrics[\"RMSE\"], metrics[\"MAE\"], metrics[\"HR\"], metrics[\"cHR\"], metrics[\"ARHR\"],\n",
        "                                      metrics[\"Coverage\"], metrics[\"Diversity\"], metrics[\"Novelty\"]))\n",
        "        else:\n",
        "            print(\"{:<10} {:<10} {:<10}\".format(\"Algorithm\", \"RMSE\", \"MAE\"))\n",
        "            for (name, metrics) in results.items():\n",
        "                print(\"{:<10} {:<10.4f} {:<10.4f}\".format(name, metrics[\"RMSE\"], metrics[\"MAE\"]))\n",
        "                \n",
        "        print(\"\\nLegend:\\n\")\n",
        "        print(\"RMSE:      Root Mean Squared Error. Lower values mean better accuracy.\")\n",
        "        print(\"MAE:       Mean Absolute Error. Lower values mean better accuracy.\")\n",
        "        if (doTopN):\n",
        "            print(\"HR:        Hit Rate; how often we are able to recommend a left-out rating. Higher is better.\")\n",
        "            print(\"cHR:       Cumulative Hit Rate; hit rate, confined to ratings above a certain threshold. Higher is better.\")\n",
        "            print(\"ARHR:      Average Reciprocal Hit Rank - Hit rate that takes the ranking into account. Higher is better.\" )\n",
        "            print(\"Coverage:  Ratio of users for whom recommendations above a certain threshold exist. Higher is better.\")\n",
        "            print(\"Diversity: 1-S, where S is the average similarity score between every possible pair of recommendations\")\n",
        "            print(\"           for a given user. Higher means more diverse.\")\n",
        "            print(\"Novelty:   Average popularity rank of recommended items. Higher means more novel.\")\n",
        "        \n",
        "    def SampleTopNRecs(self, ml, testSubject=25, k=10):\n",
        "        \n",
        "        for algo in self.algorithms:\n",
        "            print(\"\\nUsing recommender \", algo.GetName())\n",
        "            \n",
        "            print(\"\\nBuilding recommendation model...\")\n",
        "            trainSet = self.dataset.GetFullTrainSet()\n",
        "            algo.GetAlgorithm().fit(trainSet)\n",
        "            \n",
        "            print(\"Computing recommendations...\")\n",
        "            testSet = self.dataset.GetAntiTestSetForUser(testSubject)\n",
        "        \n",
        "            predictions = algo.GetAlgorithm().test(testSet)\n",
        "            \n",
        "            recommendations = []\n",
        "            \n",
        "            print (\"\\nWe recommend:\")\n",
        "            for userID, movieID, actualRating, estimatedRating, _ in predictions:\n",
        "                intMovieID = int(movieID)\n",
        "                recommendations.append((intMovieID, estimatedRating))\n",
        "            \n",
        "            recommendations.sort(key=lambda x: x[1], reverse=True)\n",
        "            \n",
        "            for ratings in recommendations[:10]:\n",
        "                print(ml.getMovieName(ratings[0]), ratings[1])\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tahC5R92LYkA"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK00GR51LYkB"
      },
      "source": [
        "**[Back to Top](#top)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ8tv2szLYkB"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9fGPcDQPbcW"
      },
      "source": [
        "<a name=\"conclusion\"></a>\n",
        "### 4.7 - Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3KFv2j0PapO"
      },
      "source": [
        "Frank's recommendation architecture includes 5 classes. Some of the functions are imported from Surpriselib and some are written from scratch, but all are interdependent to make building and testing a recommendation system easier. `RecommenderMetrics.py` holds all the functions of metrics used to assess our models. `MovieLens.py` loads up the raw MovieLens files and converts them into datasets our models can use. `EvaluatedAlgorithm.py` is the class that applies all the metrics to a given algorithm, and `EvaluationData.py` is what pre-processes the data for training and testing in our models. Last, the `Evaluator` class allows us to quickly add algorithms to train and compare with each other at the same time. When used all together, this framework makes learning and building recommendation systems easier and fun!  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLzr1J0TfYRK"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9xvMgc66fpz"
      },
      "source": [
        "**[Next Section]](https://colab.research.google.com/github/villafue/Capstone_2_MovieLens/blob/main/Notebook/4_Framework.ipynb)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcY81gt34Qd4"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brMtBr966Kln"
      },
      "source": [
        "**[Back to Top](#top)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5UQ9O5k4Ub9"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLedvFBX5EJA"
      },
      "source": [
        "**[Back to Main](https://colab.research.google.com/github/villafue/Capstone_2_MovieLens/blob/main/MovieLens.ipynb)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WM1LLNa4OQ7"
      },
      "source": [
        "***"
      ]
    }
  ]
}