{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4 Matrix Factorization and Validating Your Predictions.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPs+5mNmpNOnPTGckjo77KZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villafue/Capstone_2_Netflix/blob/main/Springboard/Tutorial/DataCamp/Building%20Recommendation%20Engines%20in%20Python/4%20Matrix%20Factorization%20and%20Validating%20Your%20Predictions/4_Matrix_Factorization_and_Validating_Your_Predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muayV4UnAwPm"
      },
      "source": [
        "# Matrix Factorization and Validating Your Predictions\r\n",
        "\r\n",
        "Understand how the sparsity of real-world datasets can impact your recommendations. Leverage the power of matrix factorization to deal with this sparsity. Explore the value of latent features and use them to better understand your data. Finally, put the models you’ve discovered to the test by learning how to validate each of the approaches you’ve learned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS7BgR7lAwHe"
      },
      "source": [
        "# Dealing with sparsity\r\n",
        "\r\n",
        "1. Dealing with sparsity\r\n",
        "\r\n",
        "Now you are capable of not only generating recommendations for any user in your dataset, but also predicting what rating users might give items they have not come across using KNN.\r\n",
        "2. Sparse matrices\r\n",
        "\r\n",
        "This works great for dense datasets in which every item has been reviewed by multiple people, which ensures that the K nearest neighbors are genuinely similar to your user. But what if the data is less full?\r\n",
        "3. Sparse matrices\r\n",
        "\r\n",
        "This is actually a common concern in real-world rating data as the number of users and items are generally quite high and the number of reviews are quite low.\r\n",
        "4. Sparse matrices\r\n",
        "\r\n",
        "We call the percentage of a DataFrame that is empty the DataFrame's sparsity. In other words, the number of empty cells over the number of cells with data.\r\n",
        "5. Measuring sparsity\r\n",
        "\r\n",
        "Let's bring back a larger version of the book rating DataFrame we used in the last chapter and find how sparse it is.\r\n",
        "6. Measuring sparsity\r\n",
        "\r\n",
        "We can check the sparsity of a DataFrame by counting the number of missing values in it using isnull dot values dot sum Finding the full number of cells in the DataFrame using dot size and then dividing the empty count by the total. Here we see that the DataFrame is only just over 1% filled, so it's quite sparse.\r\n",
        "7. Why sparsity matters\r\n",
        "\r\n",
        "Why does this matter? This can create problems if we were to use KNN with sparse data because KNN requires you to find the K nearest users that have rated the item. Take the DataFrame here.\r\n",
        "8. Why sparsity matters\r\n",
        "\r\n",
        "Let's say we wanted to estimate what User 1 would give item 5. We would find the n nearest ratings of the item,\r\n",
        "9. Why sparsity matters\r\n",
        "\r\n",
        "but in this case, there are only 2 KNN or other users that have rated the item.\r\n",
        "10. Why sparsity matters\r\n",
        "\r\n",
        "Therefore we would have to return an average of all reviews (2 in this case) because there is no other data. This does not actually take the similarities into account.\r\n",
        "11. Measuring sparsity per column\r\n",
        "\r\n",
        "You can understand the scale of this issue by simply counting the number of actual reviews for each book using notnull dot sum. We can see that a large number of books have only received one or two reviews.\r\n",
        "12. Matrix factorization\r\n",
        "\r\n",
        "So what alternatives do we have? Thankfully, we can leverage matrix factorization to deal with this problem remarkably well and create some quite interesting features while doing so.\r\n",
        "13. Matrix factorization\r\n",
        "\r\n",
        "Matrix factorization is when we decompose the user-rating matrix into the product of two lower dimensionality matrices. These matrices shown here are factors of the original matrix on the left, if you were to find the product of the two of them it would be this original matrix. By finding factors of the sparse matrix and then multiplying them together\r\n",
        "14. Matrix factorization\r\n",
        "\r\n",
        "we can be left will a fully filled matrix. We will dig into matrix factorization in the next few lessons but first we should review how matrix multiplication works.\r\n",
        "15. Matrix multiplication\r\n",
        "\r\n",
        "To multiply two rectangular matrices,\r\n",
        "16. Matrix multiplication\r\n",
        "\r\n",
        "The number of rows in the first matrix M here\r\n",
        "17. Matrix multiplication\r\n",
        "\r\n",
        "and the number of columns in the second matrix N here do not have to match\r\n",
        "18. Matrix multiplication\r\n",
        "\r\n",
        "But the number of columns of the first matrix must match the number of rows in the second.\r\n",
        "19. Matrix multiplication\r\n",
        "\r\n",
        "This results in an n by m matrix.\r\n",
        "20. Matrix multiplication\r\n",
        "\r\n",
        "This same multiplication can be performed in python using numpy's dot product function. Here we can see the dot product of matrix_A (a three by two matrix) and matrix_B (a two by three matrix),\r\n",
        "21. Matrix multiplication\r\n",
        "\r\n",
        "Is a three by three matrix.\r\n",
        "22. Let's practice!\r\n",
        "\r\n",
        "We can dig into why this is so useful soon, but let's practice what we have learned in this lesson first! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFa2H9J4Ck72"
      },
      "source": [
        "# Matrix sparsity\r\n",
        "\r\n",
        "A common challenge with real-world ratings data is that most users will not have rated most items, and most items will only have been rated by a small number of users. This results in a very empty or sparse DataFrame.\r\n",
        "\r\n",
        "In this exercise, you will calculate how sparse the movie_lens ratings data is by counting the number of occupied cells and compare it to the size of the full DataFrame. The DataFrame user_ratings_df that you have used in previous exercises, containing a row per user and a column per movie, has been loaded for you.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Count the number of non-empty cells in user_ratings_df and store the result as sparsity_count.\r\n",
        "\r\n",
        "2. Count the total number of cells in the user_ratings_df DataFrame and store it as full_count.\r\n",
        "\r\n",
        "3. Calculate the sparsity of the DataFrame by dividing the number of non-empty cells by the total number of cells and print the result.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FlF5Wgw07aC"
      },
      "source": [
        "# Count the occupied cells\r\n",
        "sparsity_count = user_ratings_df.isnull().values.sum()\r\n",
        "\r\n",
        "# Count all cells\r\n",
        "full_count = user_ratings_df.size\r\n",
        "\r\n",
        "# Find the sparsity of the DataFrame\r\n",
        "sparsity = sparsity_count / full_count\r\n",
        "print(sparsity)\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    0.9820950819672131\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5offU8RJHiy0"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Good work. As you can see, the DataFrame is over 98% empty. This means that less than 2% of the DataFrame includes any data. This suggests that it would be limited in its value for making predictions using KNN. You will soon learn how to use matrix multiplication to fill out sparse DataFrames like this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67nT21wPHnx7"
      },
      "source": [
        "# Limited data in your rows\r\n",
        "\r\n",
        "This data sparsity can cause an issue when using techniques like K-nearest neighbors as discussed in the last chapter. KNN needs to find the k most similar users that have rated an item, but if only less than or equal to k users have given an item the rating, all ratings will be the \"most similar\".\r\n",
        "\r\n",
        "In this exercise, you will count how often each movie in the user_ratings_df DataFrame has been given a rating, and then see how many have only one or two ratings.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Count the number of non-empty cells in each column of user_ratings_df and store it as occupied_count.\r\n",
        "\r\n",
        "2. Sort occupied_count from low to high. Looking at the resulting sorted Series, note the number of movies with one review.\r\n",
        "\r\n",
        "3. Create a histogram of the sorted_occupied_count Series you just created. matplotlib.pyplothas been loaded as plt.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7dHmSUfH97G"
      },
      "source": [
        "# Count the occupied cells per column\r\n",
        "occupied_count = user_ratings_df.notnull().sum()\r\n",
        "print(occupied_count)\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    '71 (2014)                                  1\r\n",
        "    'Hellboy': The Seeds of Creation (2004)     1\r\n",
        "    'Round Midnight (1986)                      2\r\n",
        "    'Salem's Lot (2004)                         1\r\n",
        "    'Til There Was You (1997)                   2\r\n",
        "                                               ..\r\n",
        "    Better Living Through Circuitry (1999)      1\r\n",
        "    Better Luck Tomorrow (2002)                 4\r\n",
        "    Better Off Dead... (1985)                  27\r\n",
        "    Better Than Chocolate (1999)                1\r\n",
        "    Better Than Sex (2000)                      1\r\n",
        "    Length: 1000, dtype: int64\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYwVYf2mIC1z"
      },
      "source": [
        "# Count the occupied cells per column\r\n",
        "occupied_count = user_ratings_df.notnull().sum()\r\n",
        "\r\n",
        "# Sort the resulting series from low to high\r\n",
        "sorted_occupied_count = occupied_count.sort_values()\r\n",
        "print(sorted_occupied_count)\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    '71 (2014)                            1\r\n",
        "    Another Earth (2011)                  1\r\n",
        "    Another Cinderella Story (2008)       1\r\n",
        "    Annie Get Your Gun (1950)             1\r\n",
        "    Anne of the Thousand Days (1969)      1\r\n",
        "                                       ... \r\n",
        "    Back to the Future (1985)           171\r\n",
        "    Aladdin (1992)                      183\r\n",
        "    Batman (1989)                       189\r\n",
        "    Apollo 13 (1995)                    201\r\n",
        "    American Beauty (1999)              204\r\n",
        "    Length: 1000, dtype: int64\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ3diZ83IPYv"
      },
      "source": [
        "# Count the occupied cells per column\r\n",
        "occupied_count = user_ratings_df.notnull().sum()\r\n",
        "\r\n",
        "# Sort the resulting series from low to high\r\n",
        "sorted_occupied_count = occupied_count.sort_values()\r\n",
        "\r\n",
        "# Plot a histogram of the values in sorted_occupied_count\r\n",
        "sorted_occupied_count.hist()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ge8zGdTIQEO"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Great job! You will notice that a very large proportion of the user_ratings_df movies have only one review. This would cause a large amount of difficulty if you were attempting to use KNN as there would not be enough valid 'neighbors'.\r\n",
        "\r\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL_EIaJnIzi6"
      },
      "source": [
        "# Matrix multiplication\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "When multiplying an 10 x 30 matrix by a 30 x 40 matrix, what size matrix is generated?\r\n",
        "\r\n",
        "Possible Answers\r\n",
        "\r\n",
        " * 40 x 10\r\n",
        "  - Incorrect. The output matrix takes the number of rows from the first matrix, and the number of columns from the second.\r\n",
        "\r\n",
        " * 30 x 30\r\n",
        "- Incorrect. While the inner two dimensions must equal each other, they do not impact the final size of the matrix.\r\n",
        "\r\n",
        " * 10 x 40\r\n",
        " - Correct! The output matrix takes the number of rows from the first matrix and the number of columns from the second. The inner two dimensions must equal each other, but they do not impact the final size of the matrix. This will become important in the next lesson as you find matrices' factors.\r\n",
        "\r\n",
        " * 10 x 30 x 40\r\n",
        " - Incorrect. When two 2-D matrices are multiplied, the resulting matrix is also two-dimensional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC51e-Z8Rt64"
      },
      "source": [
        "# Matrix factorization\r\n",
        "\r\n",
        "1. Matrix factorization\r\n",
        "\r\n",
        "Time to understand how matrix factorization can be performed and what value it brings.\r\n",
        "2. Why this helps with sparse matrices\r\n",
        "\r\n",
        "Like we mentioned in the last video, just as matrices can be multiplied together, they can be broken into their factors.\r\n",
        "3. Why this helps with sparse matrices\r\n",
        "\r\n",
        "A huge benefit of this, when performed in conjunction with recommendation systems, is that factors can be found as long as there is at least one value in every row and column. Or in other words every user has given at least one rating, and every item has been rated at least once. Why is this valuable? Because we can multiply these factors together to create a fully filled in matrix.\r\n",
        "4. Why this helps with sparse matrices\r\n",
        "\r\n",
        "That's right, it will calculate what values should be in these gaps based off of the incomplete matrix's factors. We will go into further depth about how we do this in the next lesson, but first, let's run through how we would factor our matrices.\r\n",
        "5. What matrix factorization looks like\r\n",
        "\r\n",
        "Matrix factorization breaks a matrix into two component matrices. Take a rating matrix\r\n",
        "6. What matrix factorization looks like\r\n",
        "\r\n",
        "with M users as rows\r\n",
        "7. What matrix factorization looks like\r\n",
        "\r\n",
        "and the N items they rated as the columns. Matrix factorization will break this down into one matrix with its depth equal to the number of users and one matrix with its width equal to the number of items.\r\n",
        "8. What matrix factorization looks like\r\n",
        "\r\n",
        "The number of values in the newly created dimensions shown here are called the rank of the matrix and must be equal to each other and can be decided by us. You may be wondering what these new unlabeled columns and rows represent, they are called latent features. These are the features that the matrix factorization view as mathematically the best ways to describe or sum up this data set in the least number of features.\r\n",
        "9. Latent features\r\n",
        "\r\n",
        "To explain what that entails, let's take a closer look at a small example. Here we see four users and how they have rated six books and the decomposed version of the ratings matrix. You can see that the original matrix has six columns but the first matrix that is a factor only has two columns.\r\n",
        "10. Latent features\r\n",
        "\r\n",
        "Taking a look at latent feature 1, we can see that users who gave high ratings to horror and fantasy books got relatively high values for this feature,\r\n",
        "11. Latent features\r\n",
        "\r\n",
        "while for latent feature 2, a high value appears to correspond with users who preferred romance novels. This is a simplified example, and often latent features become harder to label with larger datasets, but these are features that the matrix factorization has calculated as representing patterns in the original matrix.\r\n",
        "12. Information loss\r\n",
        "\r\n",
        "One question that might come to mind when you see these large DataFrames\r\n",
        "13. Information loss\r\n",
        "\r\n",
        "being reduced to much smaller factor matrices is, how can it do this without losing information? In reality, you can't reduce down these matrices without at least some information loss - these factors are just close approximations of the original data.\r\n",
        "14. Information loss\r\n",
        "\r\n",
        "If we were to multiply the factors back together\r\n",
        "15. Information loss\r\n",
        "\r\n",
        "we would actually see a slight difference between the first and last matrix. Even the values we had originally may be off by a small fraction. This is nothing to worry about, but worth flagging so you are not surprised when matrices do not exactly match up.\r\n",
        "16. Let's practice!\r\n",
        "\r\n",
        "Now its time to identify some real world latent features. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xdyv6twSEhP"
      },
      "source": [
        "# Identifying latent features\r\n",
        "\r\n",
        "Print original_df and user_matrix using the console. The user_matrix is one of the factors of the original_df. Based on the values in the first column of the user_matrix, what do you think the latent feature may be summarizing?\r\n",
        "\r\n",
        "Note the first row of user_matrix corresponds to User 1, the second row to User_2, and so on. Remember that latent features tend to represent underlying trends in the data and give items with these underlying trends similar scores.\r\n",
        "\r\n",
        "Possible Answers\r\n",
        "\r\n",
        "1. A preference for romance movies.\r\n",
        " - Incorrect. Users who have a high value for this latent feature have tended to score one of the two movie types mentioned high and the other low.\r\n",
        "\r\n",
        "2. A preference for horror movies.\r\n",
        " - Incorrect. In fact, users who have a high value for this latent feature have tended to score romance movies high and horror movies low.\r\n",
        "\r\n",
        "3. No preference for horror or romance movies.\r\n",
        " - Incorrect. Users who have a high value for this latent feature have tended to score one of the two movie types mentioned high and the other low."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjNCyqN7YFcd"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "\r\n",
        "# Generate the similarity matrix\r\n",
        "similarities = cosine_similarity(movie_ratings_centered)\r\n",
        "\r\n",
        "In [1]:\r\n",
        "original_df\r\n",
        "Out[1]:\r\n",
        "\r\n",
        "        Alien  Scream  Love Actually  The Notebook\r\n",
        "User_1    5.0     4.0            2.0           1.0\r\n",
        "User_2    2.0     1.0            4.0           5.0\r\n",
        "User_3    1.0     3.0            5.0           4.0\r\n",
        "User_4    4.0     5.0            1.0           2.0\r\n",
        "User_5    1.0     1.0            5.0           5.0\r\n",
        "\r\n",
        "In [2]:\r\n",
        "user_matrix\r\n",
        "Out[2]:\r\n",
        "\r\n",
        "array([[-3.9202913 ,  5.43628339],\r\n",
        "       [ 2.01738539,  6.37262296],\r\n",
        "       [ 1.46328822,  6.84850675],\r\n",
        "       [-3.88683688,  5.47272837],\r\n",
        "       [ 3.00873893,  6.54960159]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo1dbsULZSLT"
      },
      "source": [
        "# Information loss in factorization\r\n",
        "\r\n",
        "You may wonder how the factors with far fewer columns can summarize a larger DataFrame without loss. In fact, it doesn't — the factors we create are generally a close approximation of the data, as it is inevitable for some information to be lost. This means that predicted values might not be exact, but should be close enough to be useful.\r\n",
        "\r\n",
        "In this exercise, you will inspect the same original pre-factorization DataFrame from the last exercise loaded as original_df, and compare it to the product of its two factors, user_matrix and item_matrix.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Find the dot product of user_matrix and item_matrix and store it as predictions_df.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCWVav5VaULg"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "\r\n",
        "# Generate the similarity matrix\r\n",
        "similarities = cosine_similarity(movie_ratings_centered)\r\n",
        "In [1]:\r\n",
        "user_matrix\r\n",
        "Out[1]:\r\n",
        "\r\n",
        "array([[-0.92214831,  0.46868881, -3.93546218,  6.20017019],\r\n",
        "       [ 0.29451291,  1.1337195 ,  2.00684562,  7.52205181],\r\n",
        "       [-0.23338272, -1.58223229,  1.42823577,  7.4428733 ],\r\n",
        "       [ 0.93640606, -0.2676827 , -3.90282275,  6.23040608],\r\n",
        "       [-0.07751554,  0.26188815,  2.99513239,  7.67609853]])\r\n",
        "In [2]:\r\n",
        "item_matrix\r\n",
        "Out[2]:\r\n",
        "\r\n",
        "array([[-0.31315676,  0.27223577,  0.11236206, -0.6851852 ,  0.58797665],\r\n",
        "       [ 0.53506273, -0.67846698,  0.34679691, -0.33676863,  0.14038957],\r\n",
        "       [-0.58681644, -0.56753532,  0.01699735,  0.40147032,  0.41482863],\r\n",
        "       [ 0.34689509,  0.37669009,  0.48522268,  0.50094181,  0.50138271]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv7q88cBarpS"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "# Multiply the user and item matrices\r\n",
        "predictions_df = np.dot(user_matrix, item_matrix)\r\n",
        "# Inspect the recreated DataFrame\r\n",
        "print(predictions_df)\r\n",
        "\r\n",
        "# Inspect the original DataFrame and compare\r\n",
        "print(original_df)\r\n",
        "\r\n",
        "<script.py> output:\r\n",
        "    [[4.99975742 4.00002481 3.00049612 1.99995591 0.99971313]\r\n",
        "     [1.9460952  1.00551227 4.11024369 3.99020218 4.93625288]\r\n",
        "     [0.97027585 3.00303957 3.06079051 4.99459729 3.9648486 ]\r\n",
        "     [4.01506894 4.99845906 2.96918168 1.00273895 2.01782033]\r\n",
        "     [1.06960909 0.99288182 3.8576386  5.01265226 5.08231881]]\r\n",
        "   \r\n",
        "            Alien  Scream  Scream 2  Love Actually  The Notebook\r\n",
        "    User_1    5.0     4.0       3.0            2.0           1.0\r\n",
        "    User_2    2.0     1.0       4.0            4.0           5.0\r\n",
        "    User_3    1.0     3.0       3.0            5.0           4.0\r\n",
        "    User_4    4.0     5.0       3.0            1.0           2.0\r\n",
        "    User_5    1.0     1.0       4.0            5.0           5.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9hwaWPHa6Db"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Good work! Although the product of the two factors is very close to the original DataFrame, quite a bit of accuracy has been lost from reducing it to only two latent features. In the next lesson, you will learn how to find your base matrix factors yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddZgDBBha9xV"
      },
      "source": [
        "# Singular value decomposition (SVD)\r\n",
        "\r\n",
        "1. Singular value decomposition (SVD)\r\n",
        "\r\n",
        "There are many ways to find the factors of a matrix, but we will use a technique called singular value decomposition.\r\n",
        "2. What SVD does\r\n",
        "\r\n",
        "Like any matrix factorization approach, singular value decomposition finds factors for your matrix.\r\n",
        "3. What SVD does\r\n",
        "\r\n",
        "U is the user matrix\r\n",
        "4. What SVD does\r\n",
        "\r\n",
        "V transpose is the features matrix (transpose in this case means that V has been flipped over its diagonal, but we do not need to worry about that here)\r\n",
        "5. What SVD does\r\n",
        "\r\n",
        "but it also generates sigma as seen here, which is simply a diagonal matrix which can be thought of as the weights of the latent features, or how large an impact they are calculated to have.\r\n",
        "6. Prepping our data\r\n",
        "\r\n",
        "We will once again be working with a DataFrame containing book ratings called book_ratings_df. Before we get started, we should take a look at its dimensions using dot shape. For SVD to optimally work, we will need to center the data by deducting the row average from each row as we did in the third chapter. For this we find the row means for each user\r\n",
        "7. Prepping our data\r\n",
        "\r\n",
        "and subtract them from the matrix on a row-level. Once the DataFrame has been centered we can fill all the empty values with 0s without influencing the overall ratings.\r\n",
        "8. Applying SVD\r\n",
        "\r\n",
        "With the dataset normalized, we can import SVDs from scipy and apply it to our DataFrame. You can specify k, the number of latent features being generated, but in this case, we will use the default of 6. The SVD generates U, sigma, and Vt. Let's take a look at the shape of each of them. Here, we see U with the same number of rows as the original matrix, and k columns, and Vt with the same number of columns as the original matrix, and k rows.\r\n",
        "9. Applying SVD\r\n",
        "\r\n",
        "Finally, we should take a look at sigma. Note that although we expected a diagonal matrix for sigma, we get a list. This can be converted to a matrix using numpy's diag function.\r\n",
        "10. Getting the final matrix\r\n",
        "\r\n",
        "Now that we have the full factor matrices, we can multiply them together to find the full utility matrix.\r\n",
        "11. Getting the final matrix\r\n",
        "\r\n",
        "We find the dot product of U and sigma\r\n",
        "12. Getting the final matrix\r\n",
        "\r\n",
        "and then find the dot product of the result and V transpose\r\n",
        "13. Getting the final matrix\r\n",
        "\r\n",
        "to get the full matrix.\r\n",
        "14. Calculating the product in Python\r\n",
        "\r\n",
        "This can be done in Python using numpy's dot product function. First, we take the dot product of U and sigma.\r\n",
        "15. Calculating the product in Python\r\n",
        "\r\n",
        "And then the dot product of the result and V transpose. Note that these numbers look low because they have been centered.\r\n",
        "16. Add averages back\r\n",
        "\r\n",
        "Therefore we need to add the values we deducted earlier back. We extract the values from the average values series and reshape it so it can be deducted row-wise from the matrix. Upon inspection, we can see that we have been able to fill in the missing values for the DataFrame with reasonable calculated values. Below we have the original DataFrame for comparison.\r\n",
        "17. Let's practice!\r\n",
        "\r\n",
        "Now it's your turn to try it out with the movie dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIllMmStlCaG"
      },
      "source": [
        "# Normalize your data\r\n",
        "\r\n",
        "Before you can find the factors of the ratings matrix using singular value decomposition, you will need to \"de-mean\", or center it, by subtracting each row's mean from each value in that row.\r\n",
        "\r\n",
        "In this exercise, you will begin prepping the movie rating DataFrame you have been working with in order to be able to perform Singular value decomposition.\r\n",
        "\r\n",
        "user_ratings_df contains a row per user and a column for each movie and has been loaded for you.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Find the average rating each user has given across all the movies they have seen and store these values as avg_ratings.\r\n",
        "\r\n",
        "2. Subtract the row averages from their respective rows and store the result as user_ratings_centered.\r\n",
        "\r\n",
        "3. Finally, fill in all missing values in user_ratings_centered with zeros.\r\n",
        "\r\n",
        "4. Print the average of each column in user_ratings_centered to show they have been de-meaned.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af32phSml-tw"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "\r\n",
        "# Generate the similarity matrix\r\n",
        "similarities = cosine_similarity(movie_ratings_centered)\r\n",
        "In [1]:\r\n",
        "user_ratings_df.head()\r\n",
        "Out[1]:\r\n",
        "\r\n",
        "   User_'71 (2014)  User_'Hellboy': The Seeds of Creation (2004)  User_'Round Midnight (1986)  User_'Salem's Lot (2004)  User_'Til There Was You (1997)  ...  \\\r\n",
        "0              NaN                                           NaN                          NaN                       NaN                             NaN  ...   \r\n",
        "2              NaN                                           NaN                          NaN                       NaN                             NaN  ...   \r\n",
        "3              NaN                                           NaN                          NaN                       NaN                             NaN  ...   \r\n",
        "4              NaN                                           NaN                          NaN                       NaN                             NaN  ...   \r\n",
        "5              NaN                                           NaN                          NaN                       NaN                             NaN  ...   \r\n",
        "\r\n",
        "   User_Better Living Through Circuitry (1999)  User_Better Luck Tomorrow (2002)  User_Better Off Dead... (1985)  User_Better Than Chocolate (1999)  User_Better Than Sex (2000)  \r\n",
        "0                                          NaN                               NaN                             NaN                                NaN                          NaN  \r\n",
        "2                                          NaN                               NaN                             NaN                                NaN                          NaN  \r\n",
        "3                                          NaN                               NaN                             NaN                                NaN                          NaN  \r\n",
        "4                                          NaN                               NaN                             NaN                                NaN                          NaN  \r\n",
        "5                                          NaN                               NaN                             NaN                                NaN                          NaN  \r\n",
        "\r\n",
        "[5 rows x 1000 columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNqBvqAYmNEs"
      },
      "source": [
        "# Get the average rating for each user \r\n",
        "avg_ratings = user_ratings_df.mean(axis=1)\r\n",
        "\r\n",
        "# Center each user's ratings around 0\r\n",
        "user_ratings_centered = user_ratings_df.sub(avg_ratings, axis=1)\r\n",
        "\r\n",
        "# Fill in all missing values with 0s\r\n",
        "user_ratings_centered.fillna(0, inplace=True)\r\n",
        "\r\n",
        "# Print the mean of each column\r\n",
        "print(user_ratings_centered.mean(axis=1))\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    0      0.0\r\n",
        "    2      0.0\r\n",
        "    3      0.0\r\n",
        "    4      0.0\r\n",
        "    5      0.0\r\n",
        "          ... \r\n",
        "    605    0.0\r\n",
        "    606    0.0\r\n",
        "    607    0.0\r\n",
        "    608    0.0\r\n",
        "    609    0.0\r\n",
        "    Length: 589, dtype: float64\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjr2QPjKmmA9"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Great work, now that you have the data de-meaned, you will be able to perform singular value decomposition on the data and find its factors!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1CSMXOxmnVX"
      },
      "source": [
        "# Decomposing your matrix\r\n",
        "\r\n",
        "Now that you have prepped your data by centering it and filling in the remaining empty values with 0, you can get around to finding your data's factors. In this exercise, you will break the user_ratings_centered data you generated in the last exercise into 3 factors: U, sigma, and Vt.\r\n",
        "\r\n",
        " * U is a matrix with a row for each user\r\n",
        " * Vt has a column for each movie\r\n",
        " * sigma is an array of weights that you will need to convert to a diagonal matrix\r\n",
        "\r\n",
        "The user_ratings_centered that you created in the last lesson has been loaded for you.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Import svds from scipy.sparse.linalg.\r\n",
        "\r\n",
        "2. Decompose user_ratings_pivot_centered into its factor matrices: U, sigma and Vt.\r\n",
        "\r\n",
        "3.  Convert the sigma array into a diagonal matrix.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMYWnwOLtcIr"
      },
      "source": [
        "In [2]:\r\n",
        "user_ratings_centered.head()\r\n",
        "Out[2]:\r\n",
        "\r\n",
        "        '71 (2014)  'Hellboy': The Seeds of Creation (2004)  'Round Midnight (1986)  'Salem's Lot (2004)  'Til There Was You (1997)  ...  Better Living Through Circuitry (1999)  \\\r\n",
        "User_0         0.0                                      0.0                     0.0                  0.0                        0.0  ...                                     0.0   \r\n",
        "User_1         0.0                                      0.0                     0.0                  0.0                        0.0  ...                                     0.0   \r\n",
        "User_2         0.0                                      0.0                     0.0                  0.0                        0.0  ...                                     0.0   \r\n",
        "User_3         0.0                                      0.0                     0.0                  0.0                        0.0  ...                                     0.0   \r\n",
        "User_4         0.0                                      0.0                     0.0                  0.0                        0.0  ...                                     0.0   \r\n",
        "\r\n",
        "        Better Luck Tomorrow (2002)  Better Off Dead... (1985)  Better Than Chocolate (1999)  Better Than Sex (2000)  \r\n",
        "User_0                          0.0                        0.0                           0.0                     0.0  \r\n",
        "User_1                          0.0                        0.0                           0.0                     0.0  \r\n",
        "User_2                          0.0                        0.0                           0.0                     0.0  \r\n",
        "User_3                          0.0                        0.0                           0.0                     0.0  \r\n",
        "User_4                          0.0                        0.0                           0.0                     0.0  \r\n",
        "\r\n",
        "[5 rows x 1000 columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrABa1-WtwDA"
      },
      "source": [
        "# Import the required libraries \r\n",
        "from scipy.sparse.linalg import svds\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Decompose the matrix\r\n",
        "U, sigma, Vt = svds(user_ratings_centered)\r\n",
        "\r\n",
        "# Convert sigma into a diagonal matrix\r\n",
        "sigma = np.diag(sigma)\r\n",
        "print(sigma)\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    [[14.63345909  0.          0.          0.          0.          0.        ]\r\n",
        "     [ 0.         14.87028285  0.          0.          0.          0.        ]\r\n",
        "     [ 0.          0.         15.59548313  0.          0.          0.        ]\r\n",
        "     [ 0.          0.          0.         16.95141154  0.          0.        ]\r\n",
        "     [ 0.          0.          0.          0.         18.00519639  0.        ]\r\n",
        "     [ 0.          0.          0.          0.          0.         26.44511465]]\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-973c1btpRp"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Fantastic, you can use these factorized matrices to build a fully populated ratings DataFrame!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUylPQp6uAtj"
      },
      "source": [
        "# Recalculating the matrix\r\n",
        "\r\n",
        "Now that you have your three factor matrices, you can multiply them back together to get complete ratings data without missing values. In this exercise, you will use numpy's dot product function to multiply U and sigma first, then the result by Vt. You will then be able add the average ratings for each row to find your final ratings.\r\n",
        "\r\n",
        "U, sigma, Vt, avg_ratings, and user_ratings_df from the previous exercise have been loaded for you. Also, numpy has been loaded as np.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Find the dot product of the matrix U and sigma.\r\n",
        "\r\n",
        "2. Find the dot product of U_sigma and Vt and print the result.\r\n",
        "\r\n",
        "3. Reshape the values of avg_ratings and add them back onto U_sigma_Vt.\r\n",
        "\r\n",
        "4. Create a DataFrame of the results using the original index and column names from user_ratings_df.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8u93-dJuWjO"
      },
      "source": [
        "In [1]:\r\n",
        "U\r\n",
        "Out[1]:\r\n",
        "\r\n",
        "array([[-2.53981880e-02, -1.82432854e-02,  6.56053972e-03,\r\n",
        "        -1.77035634e-02, -1.46922765e-02,  8.90777301e-03],\r\n",
        "       [-1.40760083e-04,  6.49529499e-03, -1.22737785e-05,\r\n",
        "         4.57154890e-03,  2.67042018e-03,  1.80473844e-03],\r\n",
        "       [-3.45237033e-03, -1.55516654e-02,  2.08237897e-02,\r\n",
        "         2.70217774e-02, -5.81958737e-03,  6.84094183e-03],\r\n",
        "       ...,\r\n",
        "       [-1.51711203e-02,  9.20817611e-02,  1.12288448e-02,\r\n",
        "         1.58603555e-01, -1.04720236e-02,  8.10837384e-02],\r\n",
        "       [ 2.02052489e-18, -1.00020736e-18,  9.01880290e-19,\r\n",
        "         9.54975735e-19,  7.80676461e-19, -5.88917662e-19],\r\n",
        "       [ 1.10828765e-01,  8.62577174e-02, -3.26860517e-02,\r\n",
        "        -1.03669163e-02, -1.48996590e-02,  1.33897454e-01]])\r\n",
        "\r\n",
        "In [2]:\r\n",
        "sigma\r\n",
        "Out[2]:\r\n",
        "\r\n",
        "array([[14.63345909,  0.        ,  0.        ,  0.        ,  0.        ,\r\n",
        "         0.        ],\r\n",
        "       [ 0.        , 14.87028285,  0.        ,  0.        ,  0.        ,\r\n",
        "         0.        ],\r\n",
        "       [ 0.        ,  0.        , 15.59548313,  0.        ,  0.        ,\r\n",
        "         0.        ],\r\n",
        "       [ 0.        ,  0.        ,  0.        , 16.95141154,  0.        ,\r\n",
        "         0.        ],\r\n",
        "       [ 0.        ,  0.        ,  0.        ,  0.        , 18.00519639,\r\n",
        "         0.        ],\r\n",
        "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\r\n",
        "        26.44511465]])\r\n",
        "\r\n",
        "In [3]:\r\n",
        "Vt\r\n",
        "Out[3]:\r\n",
        "\r\n",
        "array([[ 2.52455153e-03, -2.89467023e-04,  3.62171175e-04, ...,\r\n",
        "        -1.55459590e-02, -8.93961713e-04, -1.38964074e-02],\r\n",
        "       [ 1.93355922e-03, -3.16764726e-04, -8.27998462e-04, ...,\r\n",
        "         4.35213024e-02,  7.09483985e-04,  4.48150544e-03],\r\n",
        "       [-6.98622189e-04,  9.61064897e-05,  3.03188008e-03, ...,\r\n",
        "        -5.76089868e-02,  4.20894958e-05, -2.91189516e-03],\r\n",
        "       [-2.03855518e-04,  8.68595437e-04,  5.53868270e-05, ...,\r\n",
        "        -1.57128941e-02,  6.86369113e-04, -1.25313447e-04],\r\n",
        "       [-2.75839979e-04,  5.33133875e-04, -2.22285817e-03, ...,\r\n",
        "        -3.79363927e-02, -1.75131301e-04,  1.25050277e-04],\r\n",
        "       [ 1.68774026e-03,  7.08714154e-04, -1.65758989e-03, ...,\r\n",
        "         3.43353303e-02, -9.54892750e-04,  9.23621220e-04]])\r\n",
        "\r\n",
        "In [4]:\r\n",
        "avg_ratings\r\n",
        "Out[4]:\r\n",
        "\r\n",
        "User_0      4.454545\r\n",
        "User_2      2.750000\r\n",
        "User_3      4.210526\r\n",
        "User_4      3.500000\r\n",
        "User_5      3.541667\r\n",
        "              ...   \r\n",
        "User_605    3.665138\r\n",
        "User_606    3.700000\r\n",
        "User_607    3.128049\r\n",
        "User_608    3.000000\r\n",
        "User_609    3.666667\r\n",
        "Length: 589, dtype: float64\r\n",
        "\r\n",
        "In [5]:\r\n",
        "user_ratings_df\r\n",
        "Out[5]:\r\n",
        "\r\n",
        "          '71 (2014)  'Hellboy': The Seeds of Creation (2004)  'Round Midnight (1986)  'Salem's Lot (2004)  'Til There Was You (1997)  ...  Better Living Through Circuitry (1999)\r\n",
        "User_0           NaN                                      NaN                     NaN                  NaN                        NaN  ...                                     NaN   \r\n",
        "User_2           NaN                                      NaN                     NaN                  NaN                        NaN  ...                                     NaN   \r\n",
        "User_3           NaN                                      NaN                     NaN                  NaN                        NaN  ...                                     NaN   \r\n",
        "User_4           NaN                                      NaN                     NaN                  NaN                        NaN  ...                                     NaN   \r\n",
        "User_5           NaN                                      NaN                     NaN                  NaN                        NaN  ...                                     NaN   \r\n",
        "...              ...                                      ...                     ...                  ...                        ...  ...                                     ...   \r\n",
        "User_605         NaN                                      NaN                     NaN                  NaN                        NaN  ...                                     NaN   \r\n",
        "User_606         NaN                                      NaN                     NaN                  NaN                        NaN  ...                                     NaN   \r\n",
        "User_607         NaN                                      NaN                     NaN                  NaN                        NaN  ...                                     NaN   \r\n",
        "User_608         NaN                                      NaN                     NaN                  NaN                        NaN  ...                                     NaN   \r\n",
        "User_609         4.0                                      NaN                     NaN                  NaN                        NaN  ...                                     NaN   \r\n",
        "\r\n",
        "          Better Luck Tomorrow (2002)  Better Off Dead... (1985)  Better Than Chocolate (1999)  Better Than Sex (2000)  \r\n",
        "User_0                            NaN                        NaN                           NaN                     NaN  \r\n",
        "User_2                            NaN                        NaN                           NaN                     NaN  \r\n",
        "User_3                            NaN                        NaN                           NaN                     NaN  \r\n",
        "User_4                            NaN                        NaN                           NaN                     NaN  \r\n",
        "User_5                            NaN                        NaN                           NaN                     NaN  \r\n",
        "...                               ...                        ...                           ...                     ...  \r\n",
        "User_605                          NaN                        NaN                           NaN                     NaN  \r\n",
        "User_606                          NaN                        NaN                           NaN                     NaN  \r\n",
        "User_607                          NaN                        NaN                           NaN                     NaN  \r\n",
        "User_608                          NaN                        NaN                           NaN                     NaN  \r\n",
        "User_609                          NaN                        NaN                           NaN                     NaN  \r\n",
        "\r\n",
        "[589 rows x 1000 columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO1LPugFuu1w"
      },
      "source": [
        "# Dot product of U and sigma\r\n",
        "U_sigma = np.dot(U, sigma)\r\n",
        "\r\n",
        "# Dot product of result and Vt\r\n",
        "U_sigma_Vt = np.dot(U_sigma, Vt)\r\n",
        "\r\n",
        "# Print the result\r\n",
        "print(U_sigma_Vt)\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    [[-1.00258090e-03 -3.13996916e-05  5.81155899e-04 ...  1.09163414e-02\r\n",
        "      -2.40503801e-04  3.87320083e-03]\r\n",
        "     [ 2.33179634e-04  9.67519507e-05 -2.62997186e-04 ...  2.84364648e-03\r\n",
        "       6.95555338e-05  5.02418651e-04]\r\n",
        "     [-5.60719501e-04  5.89305743e-04  1.11622245e-03 ... -2.49989125e-02\r\n",
        "       5.47562561e-05 -1.18340483e-03]\r\n",
        "     ...\r\n",
        "     [ 5.08768179e-03  3.40176994e-03 -3.66951278e-03 ...  9.14879300e-02\r\n",
        "       1.00813402e-03  1.03316023e-02]\r\n",
        "     [ 2.59707518e-21  8.02169966e-21  6.11346117e-20 ... -3.23958943e-18\r\n",
        "      -1.28715415e-20 -5.33145773e-19]\r\n",
        "     [ 1.30165960e-02  1.28908612e-03 -7.30301740e-03 ...  1.94495306e-01\r\n",
        "      -4.01609856e-03 -1.20456768e-02]]\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbxZS0cXu6Uu"
      },
      "source": [
        "# Dot product of U and sigma\r\n",
        "U_sigma = np.dot(U, sigma)\r\n",
        "\r\n",
        "# Dot product of result and Vt\r\n",
        "U_sigma_Vt = np.dot(U_sigma, Vt)\r\n",
        "\r\n",
        "# Add back on the row means contained in avg_ratings\r\n",
        "uncentered_ratings = U_sigma_Vt + avg_ratings.values.reshape(-1, 1)\r\n",
        "\r\n",
        "# Create DataFrame of the results\r\n",
        "calc_pred_ratings_df = pd.DataFrame(uncentered_ratings, \r\n",
        "                                    index=user_ratings_df.index,\r\n",
        "                                    columns=user_ratings_df.columns\r\n",
        "                                   )\r\n",
        "# Print both the recalculated matrix and the original \r\n",
        "print(calc_pred_ratings_df)\r\n",
        "print(original_df)\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "              '71 (2014)  'Hellboy': The Seeds of Creation (2004)  'Round Midnight (1986)  'Salem's Lot (2004)  'Til There Was You (1997)  ...  Better Living Through Circuitry (1999)  \\\r\n",
        "    User_0      4.453543                                 4.454514                4.455127             4.453897                   4.453266  ...                                4.458420   \r\n",
        "    User_2      2.750233                                 2.750097                2.749737             2.750263                   2.750053  ...                                2.748891   \r\n",
        "    User_3      4.209966                                 4.211116                4.211643             4.211584                   4.210101  ...                                4.225446   \r\n",
        "    User_4      3.500024                                 3.500017                3.498562             3.500400                   3.501181  ...                                3.493483   \r\n",
        "    User_5      3.541118                                 3.541656                3.537255             3.542141                   3.542988  ...                                3.519332   \r\n",
        "    ...              ...                                      ...                     ...                  ...                        ...  ...                                     ...   \r\n",
        "    User_605    3.662011                                 3.669527                3.665380             3.674806                   3.665815  ...                                3.735179   \r\n",
        "    User_606    3.700098                                 3.699166                3.698761             3.698140                   3.699129  ...                                3.682398   \r\n",
        "    User_607    3.133136                                 3.131451                3.124379             3.133927                   3.126718  ...                                3.099893   \r\n",
        "    User_608    3.000000                                 3.000000                3.000000             3.000000                   3.000000  ...                                3.000000   \r\n",
        "    User_609    3.679683                                 3.667956                3.659364             3.667603                   3.671040  ...                                3.540608   \r\n",
        "    \r\n",
        "              Better Luck Tomorrow (2002)  Better Off Dead... (1985)  Better Than Chocolate (1999)  Better Than Sex (2000)  \r\n",
        "    User_0                       4.454812                   4.465462                      4.454305                4.458419  \r\n",
        "    User_2                       2.750071                   2.752844                      2.750070                2.750502  \r\n",
        "    User_3                       4.208879                   4.185527                      4.210581                4.209343  \r\n",
        "    User_4                       3.502543                   3.491329                      3.499191                3.499628  \r\n",
        "    User_5                       3.548889                   3.560676                      3.540358                3.550130  \r\n",
        "    ...                               ...                        ...                           ...                     ...  \r\n",
        "    User_605                     3.664021                   3.502381                      3.663660                3.665667  \r\n",
        "    User_606                     3.701697                   3.754180                      3.700520                3.707219  \r\n",
        "    User_607                     3.128765                   3.219537                      3.129057                3.138380  \r\n",
        "    User_608                     3.000000                   3.000000                      3.000000                3.000000  \r\n",
        "    User_609                     3.677050                   3.861162                      3.662651                3.654621  \r\n",
        "    \r\n",
        "    [589 rows x 1000 columns]\r\n",
        "\r\n",
        "#Original DataFrame\r\n",
        "              '71 (2014)  'Hellboy': The Seeds of Creation (2004)  'Round Midnight (1986)  'Salem's Lot (2004)  'Til There Was You (1997)  ...  Better Living Through Circuitry (1999)  \\\r\n",
        "    User_0      0.000000                                      0.0                     0.0                  0.0                        0.0  ...                                     0.0   \r\n",
        "    User_2      0.000000                                      0.0                     0.0                  0.0                        0.0  ...                                     0.0   \r\n",
        "    User_3      0.000000                                      0.0                     0.0                  0.0                        0.0  ...                                     0.0   \r\n",
        "    User_4      0.000000                                      0.0                     0.0                  0.0                        0.0  ...                                     0.0   \r\n",
        "    User_5      0.000000                                      0.0                     0.0                  0.0                        0.0  ...                                     0.0   \r\n",
        "    ...              ...                                      ...                     ...                  ...                        ...  ...                                     ...   \r\n",
        "    User_605    0.000000                                      0.0                     0.0                  0.0                        0.0  ...                                     0.0   \r\n",
        "    User_606    0.000000                                      0.0                     0.0                  0.0                        0.0  ...                                     0.0   \r\n",
        "    User_607    0.000000                                      0.0                     0.0                  0.0                        0.0  ...                                     0.0   \r\n",
        "    User_608    0.000000                                      0.0                     0.0                  0.0                        0.0  ...                                     0.0   \r\n",
        "    User_609    0.333333                                      0.0                     0.0                  0.0                        0.0  ...                                     0.0   \r\n",
        "    \r\n",
        "              Better Luck Tomorrow (2002)  Better Off Dead... (1985)  Better Than Chocolate (1999)  Better Than Sex (2000)  \r\n",
        "    User_0                            0.0                        0.0                           0.0                     0.0  \r\n",
        "    User_2                            0.0                        0.0                           0.0                     0.0  \r\n",
        "    User_3                            0.0                        0.0                           0.0                     0.0  \r\n",
        "    User_4                            0.0                        0.0                           0.0                     0.0  \r\n",
        "    User_5                            0.0                        0.0                           0.0                     0.0  \r\n",
        "    ...                               ...                        ...                           ...                     ...  \r\n",
        "    User_605                          0.0                        0.0                           0.0                     0.0  \r\n",
        "    User_606                          0.0                        0.0                           0.0                     0.0  \r\n",
        "    User_607                          0.0                        0.0                           0.0                     0.0  \r\n",
        "    User_608                          0.0                        0.0                           0.0                     0.0  \r\n",
        "    User_609                          0.0                        0.0                           0.0                     0.0  \r\n",
        "    \r\n",
        "    [589 rows x 1000 columns]\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eDdlng-vle5"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Great job! As you can see, although the initial DataFrame had large gaps in the data, the recalculated one is full, and therefore could be used to create better recommendations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6l74ku9vr4I"
      },
      "source": [
        "# Making recommendations with SVD\r\n",
        "\r\n",
        "Now that you have the recalculated matrix with all of its gaps filled in, the next step is to use it to generate predictions and recommendations.\r\n",
        "\r\n",
        "Using calc_pred_ratings_df that you generated in the last exercise, with all rows and columns filled, find the movies that User_5 is most likely to enjoy.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Find the highest ranked movies for User_5 by sorting all the reviews generated for User_5 from high to low.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA8vUBcNv2-e"
      },
      "source": [
        "In [1]:\r\n",
        "calc_pred_ratings_df\r\n",
        "Out[1]:\r\n",
        "\r\n",
        "          '71 (2014)  'Hellboy': The Seeds of Creation (2004)  'Round Midnight (1986)  'Salem's Lot (2004)  'Til There Was You (1997)  ...  Better Living Through Circuitry (1999)  \r\n",
        "User_0      4.453543                                 4.454514                4.455127             4.453897                   4.453266  ...                                4.458420   \r\n",
        "User_2      2.750233                                 2.750097                2.749737             2.750263                   2.750053  ...                                2.748891   \r\n",
        "User_3      4.209966                                 4.211116                4.211643             4.211584                   4.210101  ...                                4.225446   \r\n",
        "User_4      3.500024                                 3.500017                3.498562             3.500400                   3.501181  ...                                3.493483   \r\n",
        "User_5      3.541118                                 3.541656                3.537255             3.542141                   3.542988  ...                                3.519332   \r\n",
        "...              ...                                      ...                     ...                  ...                        ...  ...                                     ...   \r\n",
        "User_605    3.662011                                 3.669527                3.665380             3.674806                   3.665815  ...                                3.735179   \r\n",
        "User_606    3.700098                                 3.699166                3.698761             3.698140                   3.699129  ...                                3.682398   \r\n",
        "User_607    3.133136                                 3.131451                3.124379             3.133927                   3.126718  ...                                3.099893   \r\n",
        "User_608    3.000000                                 3.000000                3.000000             3.000000                   3.000000  ...                                3.000000   \r\n",
        "User_609    3.679683                                 3.667956                3.659364             3.667603                   3.671040  ...                                3.540608   \r\n",
        "\r\n",
        "          Better Luck Tomorrow (2002)  Better Off Dead... (1985)  Better Than Chocolate (1999)  Better Than Sex (2000)  \r\n",
        "User_0                       4.454812                   4.465462                      4.454305                4.458419  \r\n",
        "User_2                       2.750071                   2.752844                      2.750070                2.750502  \r\n",
        "User_3                       4.208879                   4.185527                      4.210581                4.209343  \r\n",
        "User_4                       3.502543                   3.491329                      3.499191                3.499628  \r\n",
        "User_5                       3.548889                   3.560676                      3.540358                3.550130  \r\n",
        "...                               ...                        ...                           ...                     ...  \r\n",
        "User_605                     3.664021                   3.502381                      3.663660                3.665667  \r\n",
        "User_606                     3.701697                   3.754180                      3.700520                3.707219  \r\n",
        "User_607                     3.128765                   3.219537                      3.129057                3.138380  \r\n",
        "User_608                     3.000000                   3.000000                      3.000000                3.000000  \r\n",
        "User_609                     3.677050                   3.861162                      3.662651                3.654621  \r\n",
        "\r\n",
        "[589 rows x 1000 columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MwTac4JwKvC"
      },
      "source": [
        "# Sort the ratings of User 5 from high to low\r\n",
        "user_5_ratings = calc_pred_ratings_df.loc['User_5',:].sort_values(ascending=False)\r\n",
        "\r\n",
        "print(user_5_ratings)\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    Back to the Future (1985)                       4.163975\r\n",
        "    Apollo 13 (1995)                                4.163789\r\n",
        "    Beauty and the Beast (1991)                     4.087964\r\n",
        "    Aladdin (1992)                                  3.962188\r\n",
        "    Airplane! (1980)                                3.812181\r\n",
        "                                                      ...   \r\n",
        "    American Psycho (2000)                          3.367901\r\n",
        "    Aliens (1986)                                   3.351066\r\n",
        "    Austin Powers: The Spy Who Shagged Me (1999)    3.327153\r\n",
        "    Ace Ventura: When Nature Calls (1995)           2.982156\r\n",
        "    Ace Ventura: Pet Detective (1994)               2.628886\r\n",
        "    Name: User_5, Length: 1000, dtype: float64\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3dzLqGywWvY"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Good work, now you can make predictions for what any user would give any item in the DataFrame even if the original DataFrame is very sparse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H4IUcr3wbri"
      },
      "source": [
        "# Validating your predictions\r\n",
        "\r\n",
        "1. Validating your predictions\r\n",
        "\r\n",
        "You have now worked through a few different approaches in predicting how a user may feel about an item they have not seen and from that are able to recommend new items to them that they are most likely to enjoy. For all of these approaches, we have inspected the data to see if it looked correct, but we have not discussed how to measure how accurate these predictions are.\r\n",
        "2. Hold-out sets\r\n",
        "\r\n",
        "What makes recommendation engines a little different when measuring predictions is that in more traditional machine learning models,\r\n",
        "3. Hold-out sets\r\n",
        "\r\n",
        "you are trying to predict a single feature or column,\r\n",
        "4. Hold-out sets\r\n",
        "\r\n",
        "but with recommendation engines, what you are trying to predict is far more inconsistent.\r\n",
        "5. Hold-out sets\r\n",
        "\r\n",
        "Almost every user has reviewed different items, and each item has received reviews from different groups of users.\r\n",
        "6. Hold-out sets\r\n",
        "\r\n",
        "For this reason, we cannot split our holdout set in the same way that we can for typical machine learning. In those cases, we would just split off a proportion of the row and use them to test our predictions as you see on the left.\r\n",
        "7. Hold-out sets\r\n",
        "\r\n",
        "For recommendation engines, on the other hand, we need to remove a different chunk of the DataFrame, as seen on the right.\r\n",
        "8. Separating the hold-out set\r\n",
        "\r\n",
        "This can be done in Python by first extracting the area you wish to compare. In our case, we will focus on the top left-hand corner of our base DataFrame consisting of the first 20 rows and the first 100 columns. Here they are selected using iloc. We then blank out the area with NaNs, as this is what we will be predicting. We then repeat the factorization from the last lesson to fill out the full DataFrame, and take a subset of the predicted DataFrame that you want to compare against. We now have the predicted values, and the original actual values that were not used to predict\r\n",
        "9. Masking the hold-out set\r\n",
        "\r\n",
        "As we only want to compare the values that did exist, we mask the DataFrame to only compare non-missing fields. Now if we take a look at the masked original DataFrame and the predicted one, we can see that only the values we want to compare are present.\r\n",
        "10. Introducing RMSE (root mean squared error)\r\n",
        "\r\n",
        "The metric most commonly used to measure how good a model is at predicting a recommendation is called root mean square error or RMSE for short.\r\n",
        "11. Introducing RMSE (root mean squared error)\r\n",
        "\r\n",
        "With RMSE, we first calculate how far from the ground truth each prediction was (this is the error part in RMSE).\r\n",
        "12. Introducing RMSE (root mean squared error)\r\n",
        "\r\n",
        "We then square this as we only care about how wrong it is, not in what direction.\r\n",
        "13. Introducing RMSE (root mean squared error)\r\n",
        "\r\n",
        "We then find the average square error. The sum of all the errors divided by the total number, using our example shown here would be 5 over 3.\r\n",
        "14. Introducing RMSE (root mean squared error)\r\n",
        "\r\n",
        "We then find the square root of this value.\r\n",
        "15. Introducing RMSE (root mean squared error)\r\n",
        "\r\n",
        "This gives us a good measure of how close a set of predictions are to the actual values, and is very useful to compare between models.\r\n",
        "16. RMSE in Python\r\n",
        "\r\n",
        "The Root mean square error can be found in Python using sklearn's mean_squared_error function, taking the two sets of data you want to compare as its first and second argument. We set the optional argument squared to False so we calculate the root mean square error as opposed to the mean square error.\r\n",
        "17. Let's practice!\r\n",
        "\r\n",
        "Great, now you know how to measure how good your recommendations are, let's compare two different approaches. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgu-kXjgz3uq"
      },
      "source": [
        "# Calculating RMSE\r\n",
        "\r\n",
        "The following data has been loaded in the DataFrame predictions. Either manually, or using the Python console, calculate what the root mean square error (RMSE) of these predictions is.\r\n",
        "\r\n",
        "| Predicted |\tActual |\r\n",
        "|----------|---------|\r\n",
        "|3 |\t4 |\r\n",
        "|2 |\t1 |\r\n",
        "|5 |\t3 |\r\n",
        "\r\n",
        "Possible Answers\r\n",
        "\r\n",
        "* 3 / root 6\r\n",
        " - Incorrect: Remember to find the mean (or the average) of the value representing the number of items would need to be the denominator.\r\n",
        "\r\n",
        "* root ( 3 / 6)\r\n",
        " - Incorrect: Remember to find the mean (or the average) of the value representing the number of items would need to be the denominator.\r\n",
        "\r\n",
        "* 6 / root 3\r\n",
        " - Incorrect: Almost. Remember that you need to find the square root of the full equation, not just the denominator.\r\n",
        "\r\n",
        "* root (6 / 3)\r\n",
        " - Correct! RMSE takes the square root of the average error squared. Now you can apply this to your prediction data to see how close it was to the actual values.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_OOC7S12IBR"
      },
      "source": [
        "# Comparing recommendation methods\r\n",
        "\r\n",
        "In this course, you have predicted how you believe a user would rate movies they have not seen using multiple different methods (basic average ratings, KNN, matrix factorization). In this final exercise, you'll work through a comparison of the averaged ratings and matrix factorization using the mean_squared_error() as the measure of how well they are performing. The predictions based on averages have been loaded as avg_pred_ratings_df while the calculated predictions have been loaded as calc_pred_ratings_df. The ground truth values have been loaded as act_ratings_df.\r\n",
        "\r\n",
        "Finally, the mean_squared_error() function has been imported for your use from sklearn.metrics.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Extract rows 0-20 and columns 0-100 (the areas that you want to compare) in the act_ratings_df, avg_pred_ratings_df, and calc_pred_ratings_df DataFrames.\r\n",
        "\r\n",
        "2. Create a mask of the actual_values DataFrame that targets only non-empty cells.\r\n",
        "\r\n",
        "3. Find the mean squared error between the two predictions and the ground truth values.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQKSjDcC1yQ6"
      },
      "source": [
        "In [1]:\r\n",
        "avg_pred_ratings_df\r\n",
        "Out[1]:\r\n",
        "\r\n",
        "            1         2         3         4         5  ...       454       455       456       457       458\r\n",
        "0   -0.000273 -0.000273 -0.000273 -0.000273 -0.000273  ... -0.000273 -0.000273 -0.000273  0.629191 -0.000273\r\n",
        "1   -0.000525 -0.000525 -0.000525 -0.000525 -0.000525  ... -0.000525 -0.000525 -0.000525 -0.000525 -0.000525\r\n",
        "2    0.000018  0.000018  0.000018  0.000018  0.000018  ...  0.000018  0.000018  0.000018  0.000018  0.000018\r\n",
        "3    0.000387  0.000387  0.000387  0.000387  0.000387  ...  0.000387  0.000387  0.000387  1.421440  0.000387\r\n",
        "4    0.000412  0.000412  0.000412  0.000412  0.000412  ...  0.000412  0.000412  0.000412  0.444857  0.000412\r\n",
        "..        ...       ...       ...       ...       ...  ...       ...       ...       ...       ...       ...\r\n",
        "395  1.857322  0.000179  0.000179  0.000179  0.000179  ...  0.000179  0.000179  0.000179  0.000179  0.000179\r\n",
        "396 -0.000197 -0.000197 -0.000197 -0.000197 -0.000197  ... -0.000197 -0.000197 -0.000197 -0.000197 -0.000197\r\n",
        "397  0.000000  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000\r\n",
        "398  0.055099 -0.000457 -0.000457 -0.000457 -0.000457  ... -0.000457 -0.000457 -0.000457 -0.000457 -0.000457\r\n",
        "399 -0.000086 -0.000086 -0.000086 -0.000086 -0.000086  ... -0.000086 -0.000086 -0.000086 -0.000086 -0.000086\r\n",
        "\r\n",
        "[400 rows x 400 columns]\r\n",
        "\r\n",
        "In [2]:\r\n",
        "calc_pred_ratings_df\r\n",
        "Out[2]:\r\n",
        "\r\n",
        "            0         1         2         3         4  ...       395       396       397       398       399\r\n",
        "0    6.670980  5.209519  5.046434  4.355463  4.477881  ...  4.298304  4.569487  4.482124  6.508589  4.352706\r\n",
        "1    4.054860  4.009719  3.854787  3.949990  3.928394  ...  4.047494  3.915937  3.950807  4.074458  3.948838\r\n",
        "2    2.501992  2.490222  2.511609  2.484290  2.475056  ...  2.451567  2.480562  2.492486  2.541184  2.491379\r\n",
        "3    4.606159  3.598486  3.775842  3.605875  3.632134  ...  3.765071  3.564204  3.706584  4.353608  3.609261\r\n",
        "4    4.520100  4.249675  3.872521  3.604933  3.931391  ...  4.973189  3.726100  3.549327  5.434189  3.613979\r\n",
        "..        ...       ...       ...       ...       ...  ...       ...       ...       ...       ...       ...\r\n",
        "395  3.506448  3.286905  3.175207  3.149274  3.193238  ...  3.298484  3.179871  3.149476  3.444694  3.133384\r\n",
        "396  3.896269  3.858694  3.934466  3.960730  3.931922  ...  3.966246  3.912275  3.977568  3.939699  3.971941\r\n",
        "397  4.530719  4.308165  4.230908  4.251794  4.261548  ...  4.183252  4.271312  4.263262  4.184225  4.243795\r\n",
        "398  5.135655  4.296887  3.932518  3.948227  3.972904  ...  4.101042  4.018191  3.964400  4.755848  3.841128\r\n",
        "399  5.495461  4.623944  4.350521  4.521719  4.415405  ...  4.821735  4.405674  4.579227  5.577847  4.432656\r\n",
        "\r\n",
        "[400 rows x 400 columns]\r\n",
        "\r\n",
        "In [3]:\r\n",
        "act_ratings_df\r\n",
        "Out[3]:\r\n",
        "\r\n",
        "       1   2    3   4   5  ...  454  455  456  457  458\r\n",
        "0    4.0 NaN  4.0 NaN NaN  ...  NaN  NaN  NaN  5.0  NaN\r\n",
        "1    NaN NaN  NaN NaN NaN  ...  NaN  NaN  NaN  NaN  NaN\r\n",
        "2    NaN NaN  NaN NaN NaN  ...  NaN  NaN  NaN  NaN  NaN\r\n",
        "3    NaN NaN  NaN NaN NaN  ...  NaN  NaN  NaN  5.0  NaN\r\n",
        "4    4.0 NaN  NaN NaN NaN  ...  NaN  NaN  NaN  4.0  NaN\r\n",
        "..   ...  ..  ...  ..  ..  ...  ...  ...  ...  ...  ...\r\n",
        "395  5.0 NaN  NaN NaN NaN  ...  NaN  NaN  NaN  NaN  NaN\r\n",
        "396  NaN NaN  NaN NaN NaN  ...  NaN  NaN  NaN  NaN  NaN\r\n",
        "397  NaN NaN  NaN NaN NaN  ...  NaN  NaN  NaN  NaN  NaN\r\n",
        "398  4.0 NaN  NaN NaN NaN  ...  NaN  NaN  NaN  NaN  NaN\r\n",
        "399  NaN NaN  NaN NaN NaN  ...  NaN  NaN  NaN  NaN  NaN\r\n",
        "\r\n",
        "[400 rows x 400 columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDY36nfL4WkL"
      },
      "source": [
        "# Extract the ground truth to compare your predictions against\r\n",
        "actual_values = act_ratings_df.iloc[:20, :100].values\r\n",
        "avg_values = avg_pred_ratings_df.iloc[:20, :100].values\r\n",
        "predicted_values = calc_pred_ratings_df.iloc[:20, :100].values\r\n",
        "\r\n",
        "# Create a mask of actual_values to only look at the non-missing values in the ground truth\r\n",
        "mask = ~np.isnan(actual_values)\r\n",
        "\r\n",
        "# Print the performance of both predictions and compare\r\n",
        "print(mean_squared_error(actual_values[mask], avg_values[mask], squared=False))\r\n",
        "print(mean_squared_error(actual_values[mask], predicted_values[mask], squared=False))\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    3.622399787897257\r\n",
        "    1.6415371001781351\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7OqUpTT4aNb"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Fantastic! You can see that the predictions you created with your newly learned skills generate values that are far more accurate than the baseline averages. Congratulations! You did it! You’re now able to create recommendation engines in Python. Tweet us your feedback and tell us what you think."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STM9--ot4gvt"
      },
      "source": [
        "# Wrap up\r\n",
        "\r\n",
        "1. Wrap up\r\n",
        "\r\n",
        "Congratulations! You have reached the end of this course on recommendation engines. Let's review what you have learned.\r\n",
        "2. Non-personalized models\r\n",
        "\r\n",
        "In chapter 1, you learned what recommendation engines are and how they can be used, and explored how even very basic, non-personalized models can be useful for things like recommending items that have been bought together.\r\n",
        "3. Content-based models\r\n",
        "\r\n",
        "In chapter two, you progressed to content-based modeling and learned how it is valuable in cases where you have a lot of information about the items you want to recommend, but maybe not as much about the users in your data.\r\n",
        "4. Collaborative filtering\r\n",
        "\r\n",
        "Then in Chapter 3, you moved from knowing a lot about our items to instead having a good understanding of what users liked what, and used collaborative filtering to find new items that would be of interest to our users.\r\n",
        "5. Matrix factorization\r\n",
        "\r\n",
        "Finally, in Chapter 4 you learned about how matrix factorization can be used on very sparse datasets, a likely occurrence in real data, to not only generate recommendations but also to learn more about your data using latent features.\r\n",
        "6. Congratulations!\r\n",
        "\r\n",
        "Well done, you are now in the position to begin creating your own recommendation engines using your own data! Good luck! "
      ]
    }
  ]
}